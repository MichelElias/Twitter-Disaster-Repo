{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-twitter in /opt/conda/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from python-twitter) (2.21.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.6/site-packages (from python-twitter) (1.0.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from python-twitter) (0.17.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->python-twitter) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->python-twitter) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->python-twitter) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->python-twitter) (2018.11.29)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib->python-twitter) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.6/site-packages (2.7.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import psycopg2 as pg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from psycopg2.extras import RealDictCursor, Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run sql_pw.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_cur_to_db(dbname=DBNAME, dict_cur=None):\n",
    "    ''' \n",
    "    Returns both a connection and a cursor object for your database\n",
    "    '''\n",
    "\n",
    "    con = pg2.connect(host=IP_ADDRESS, #allows you to navigate db\n",
    "                  dbname=dbname,\n",
    "                  user=USER,\n",
    "                  password=PASSWORD)\n",
    "    if dict_cur:\n",
    "        cur = con.cursor(cursor_factory=RealDictCursor)\n",
    "    else:\n",
    "        cur = con.cursor()\n",
    "    return con, cur\n",
    "    \n",
    "def execute_query(query, dbname=DBNAME, dict_cur=None, command=False):\n",
    "    '''\n",
    "    Executes a query directly to a database, without having to create a cursor and connection each time. \n",
    "    '''\n",
    "    con, cur = con_cur_to_db(dbname, dict_cur)\n",
    "    cur.execute(f'{query}')\n",
    "    if not command:\n",
    "        data = cur.fetchall()\n",
    "        con.close()\n",
    "        return data\n",
    "    con.commit() #sends to server\n",
    "    con.close() #closes server connection\n",
    "\n",
    "def insert_entry_json(data, tablename=None):\n",
    "    con, cur = con_cur_to_db()\n",
    "    for x in data:\n",
    "        cur.execute(f'INSERT INTO {tablename} (data) VALUES ({Json(x)});')\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_keys = {\n",
    "    'consumer_key':        'x2QffrlOw2v0oJjakKYIHsR6h',\n",
    "    'consumer_secret':     'xViQ3VoIMUqWrdsXS4hTL0GuBVqPQx8lkxqxdibjEDLdinflnC',\n",
    "    'access_token_key':    '319864871-2UQ33shLpkYA9ZcZfA7OXSYKrespIjwEZNS4tOHn',\n",
    "    'access_token_secret': 'wpLbZp01SCcxscviBORW9bpTROCvmLe1PJ9006sjXdaQv'\n",
    "}\n",
    "\n",
    "api = twitter.Api(consumer_key         =   twitter_keys['consumer_key'],\n",
    "                  consumer_secret      =   twitter_keys['consumer_secret'],\n",
    "                  access_token_key     =   twitter_keys['access_token_key'],\n",
    "                  access_token_secret  =   twitter_keys['access_token_secret'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Sat Jun 18 21:53:40 +0000 2011\", \"default_profile\": true, \"default_profile_image\": true, \"followers_count\": 2, \"friends_count\": 2, \"id\": 319864871, \"id_str\": \"319864871\", \"lang\": \"en\", \"name\": \"Mintaek Hong\", \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"MintaekH\", \"status\": {\"created_at\": \"Sun Mar 04 14:41:49 +0000 2012\", \"id\": 176316523554684928, \"id_str\": \"176316523554684928\", \"in_reply_to_screen_name\": \"gnltjs1343\", \"in_reply_to_user_id\": 400616621, \"lang\": \"ko\", \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"text\": \"@gnltjs1343 \\ub300\\uace1\\ucd08\\ub4f1\\ud559\\uad50 \\ud76c\\uc120 \\ub9de\\uc9c0?\"}, \"statuses_count\": 1}\n"
     ]
    }
   ],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamTweets(city, geocode, term, before, count, sql_db):\n",
    "    for i in range(1,8):\n",
    "        year, month, day = before.split('-')\n",
    "        day = int(day)\n",
    "        day-=1\n",
    "        day = str(day).zfill(2)\n",
    "        date = year + month + day\n",
    "        after = datetime.strptime(date, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "        \n",
    "        results = api.GetSearch(\n",
    "            term = term,\n",
    "            geocode = geocode,\n",
    "            return_json = True\n",
    "        )\n",
    "\n",
    "        insert_entry_json(results['statuses'], sql_db)\n",
    "        before = after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchterms = ['man of the woods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_repeater(city, geocode, term, start, repeats=15, count=100, sql_db='raw_tweets'):\n",
    "    for i in range(repeats):\n",
    "        before = start\n",
    "        \n",
    "        streamTweets(city, geocode, term, before, count, sql_db)\n",
    "        print(f'Loop {i+1} complete. Raw tweets pushed to {sql_db}.')\n",
    "        time.sleep(40)\n",
    "        \n",
    "    print('All tweets pulled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 1 complete. Raw tweets pushed to raw_tweets.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-614bc3757ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                sql_db='raw_tweets')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-4da29e087e5e>\u001b[0m in \u001b[0;36mtweet_repeater\u001b[0;34m(city, geocode, term, start, repeats, count, sql_db)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstreamTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeocode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loop {i+1} complete. Raw tweets pushed to {sql_db}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All tweets pulled.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweet_repeater(city='cha', \n",
    "               geocode='35.2271,-80.8431,10mi', \n",
    "               term = 'man of the woods',\n",
    "               start='2019-01-08', \n",
    "               repeats=100, \n",
    "               count=100, \n",
    "               sql_db='raw_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT data->>'text'\n",
    "FROM raw_tweets;\n",
    "\"\"\"\n",
    "response = execute_query(query, dict_cur=True)\n",
    "\n",
    "df = pd.DataFrame(response).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20694</th>\n",
       "      <td>@gulzaarAvi @kvnghakeem1 SDV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20695</th>\n",
       "      <td>Says he's gonna teach me just what fast is, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20696</th>\n",
       "      <td>@ShimminyKricket Spot on nerdy! For those that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20697</th>\n",
       "      <td>One of may fav shows on TV is back tonight. ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20698</th>\n",
       "      <td>and today i have a new job .. God come onnnnnn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "20694                       @gulzaarAvi @kvnghakeem1 SDV\n",
       "20695  Says he's gonna teach me just what fast is, sa...\n",
       "20696  @ShimminyKricket Spot on nerdy! For those that...\n",
       "20697  One of may fav shows on TV is back tonight. ht...\n",
       "20698  and today i have a new job .. God come onnnnnn..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20699, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def processTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Eliminate hastags\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Remove @ signs\n",
    "    tweet = re.sub('@', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = [i for i in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My article on Maori Davenport and the unreasonable ruling of Steve Savarese. Savarese’s comments are revealing, and… https://t.co/Klko7QAn0f',\n",
       " 'Today’s AHSAA statement on Maori Davenport not only contains false and misleading assertions, it paints AHSAA as a… https://t.co/GDqhyeHbfQ',\n",
       " 'Today’s AHSAA statement on Maori Davenport contains false and misleading assertions. The Illinois player DID deposi… https://t.co/k5l2cpULI8',\n",
       " 'Power https://t.co/qcSkhvxfXO',\n",
       " '@amazon @AmazonHelp @USPS @USPSHelp - PLEASE you guys.  Again. Please deal with the manners of these delivery peopl… https://t.co/Y510McExtq']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column of processed tweets utilizing the created function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed'] = [processTweet(i) for i in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My article on Maori Davenport and the unreason...</td>\n",
       "      <td>my article on maori davenport and the unreason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today’s AHSAA statement on Maori Davenport not...</td>\n",
       "      <td>today’s ahsaa statement on maori davenport not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Today’s AHSAA statement on Maori Davenport con...</td>\n",
       "      <td>today’s ahsaa statement on maori davenport con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Power https://t.co/qcSkhvxfXO</td>\n",
       "      <td>power URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@amazon @AmazonHelp @USPS @USPSHelp - PLEASE y...</td>\n",
       "      <td>amazon amazonhelp usps uspshelp - please you g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  My article on Maori Davenport and the unreason...   \n",
       "1  Today’s AHSAA statement on Maori Davenport not...   \n",
       "2  Today’s AHSAA statement on Maori Davenport con...   \n",
       "3                      Power https://t.co/qcSkhvxfXO   \n",
       "4  @amazon @AmazonHelp @USPS @USPSHelp - PLEASE y...   \n",
       "\n",
       "                                           processed  \n",
       "0  my article on maori davenport and the unreason...  \n",
       "1  today’s ahsaa statement on maori davenport not...  \n",
       "2  today’s ahsaa statement on maori davenport con...  \n",
       "3                                          power URL  \n",
       "4  amazon amazonhelp usps uspshelp - please you g...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /opt/conda/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_processed'] = df['processed'].map(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "      <th>clean_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My article on Maori Davenport and the unreason...</td>\n",
       "      <td>my article on maori davenport and the unreason...</td>\n",
       "      <td>[my, article, on, maori, davenport, and, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today’s AHSAA statement on Maori Davenport not...</td>\n",
       "      <td>today’s ahsaa statement on maori davenport not...</td>\n",
       "      <td>[today, s, ahsaa, statement, on, maori, davenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Today’s AHSAA statement on Maori Davenport con...</td>\n",
       "      <td>today’s ahsaa statement on maori davenport con...</td>\n",
       "      <td>[today, s, ahsaa, statement, on, maori, davenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Power https://t.co/qcSkhvxfXO</td>\n",
       "      <td>power URL</td>\n",
       "      <td>[power, URL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@amazon @AmazonHelp @USPS @USPSHelp - PLEASE y...</td>\n",
       "      <td>amazon amazonhelp usps uspshelp - please you g...</td>\n",
       "      <td>[amazon, amazonhelp, usps, uspshelp, please, y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  My article on Maori Davenport and the unreason...   \n",
       "1  Today’s AHSAA statement on Maori Davenport not...   \n",
       "2  Today’s AHSAA statement on Maori Davenport con...   \n",
       "3                      Power https://t.co/qcSkhvxfXO   \n",
       "4  @amazon @AmazonHelp @USPS @USPSHelp - PLEASE y...   \n",
       "\n",
       "                                           processed  \\\n",
       "0  my article on maori davenport and the unreason...   \n",
       "1  today’s ahsaa statement on maori davenport not...   \n",
       "2  today’s ahsaa statement on maori davenport con...   \n",
       "3                                          power URL   \n",
       "4  amazon amazonhelp usps uspshelp - please you g...   \n",
       "\n",
       "                                     clean_processed  \n",
       "0  [my, article, on, maori, davenport, and, the, ...  \n",
       "1  [today, s, ahsaa, statement, on, maori, davenp...  \n",
       "2  [today, s, ahsaa, statement, on, maori, davenp...  \n",
       "3                                       [power, URL]  \n",
       "4  [amazon, amazonhelp, usps, uspshelp, please, y...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemm_clean_processed'] = df['clean_processed'].map(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "      <th>clean_processed</th>\n",
       "      <th>lemm_clean_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My article on Maori Davenport and the unreason...</td>\n",
       "      <td>my article on maori davenport and the unreason...</td>\n",
       "      <td>[my, article, on, maori, davenport, and, the, ...</td>\n",
       "      <td>my article on maori davenport and the unreason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today’s AHSAA statement on Maori Davenport not...</td>\n",
       "      <td>today’s ahsaa statement on maori davenport not...</td>\n",
       "      <td>[today, s, ahsaa, statement, on, maori, davenp...</td>\n",
       "      <td>today s ahsaa statement on maori davenport not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Today’s AHSAA statement on Maori Davenport con...</td>\n",
       "      <td>today’s ahsaa statement on maori davenport con...</td>\n",
       "      <td>[today, s, ahsaa, statement, on, maori, davenp...</td>\n",
       "      <td>today s ahsaa statement on maori davenport con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Power https://t.co/qcSkhvxfXO</td>\n",
       "      <td>power URL</td>\n",
       "      <td>[power, URL]</td>\n",
       "      <td>power URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@amazon @AmazonHelp @USPS @USPSHelp - PLEASE y...</td>\n",
       "      <td>amazon amazonhelp usps uspshelp - please you g...</td>\n",
       "      <td>[amazon, amazonhelp, usps, uspshelp, please, y...</td>\n",
       "      <td>amazon amazonhelp usps uspshelp please you guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  My article on Maori Davenport and the unreason...   \n",
       "1  Today’s AHSAA statement on Maori Davenport not...   \n",
       "2  Today’s AHSAA statement on Maori Davenport con...   \n",
       "3                      Power https://t.co/qcSkhvxfXO   \n",
       "4  @amazon @AmazonHelp @USPS @USPSHelp - PLEASE y...   \n",
       "\n",
       "                                           processed  \\\n",
       "0  my article on maori davenport and the unreason...   \n",
       "1  today’s ahsaa statement on maori davenport not...   \n",
       "2  today’s ahsaa statement on maori davenport con...   \n",
       "3                                          power URL   \n",
       "4  amazon amazonhelp usps uspshelp - please you g...   \n",
       "\n",
       "                                     clean_processed  \\\n",
       "0  [my, article, on, maori, davenport, and, the, ...   \n",
       "1  [today, s, ahsaa, statement, on, maori, davenp...   \n",
       "2  [today, s, ahsaa, statement, on, maori, davenp...   \n",
       "3                                       [power, URL]   \n",
       "4  [amazon, amazonhelp, usps, uspshelp, please, y...   \n",
       "\n",
       "                                lemm_clean_processed  \n",
       "0  my article on maori davenport and the unreason...  \n",
       "1  today s ahsaa statement on maori davenport not...  \n",
       "2  today s ahsaa statement on maori davenport con...  \n",
       "3                                          power URL  \n",
       "4  amazon amazonhelp usps uspshelp please you guy...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_clean_processed_list = [i for i in df['lemm_clean_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize countvectorizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_df will remember words that are in a document at least once. For example, if min_df = 3, will only remember words that are a part of 3 documents in a corpus. max_df refers to the %-age of documents within a corpus that a word occurs in. For example, if max_df = 0.9, words that occur in more than 90% of my documents will be ignored. By default, max_df is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range = (1,2),\n",
    "                     stop_words = 'english',\n",
    "                     min_df = 15,\n",
    "                     max_df = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = pd.SparseDataFrame(cv.fit_transform(df['lemm_clean_processed']), \n",
    "                     columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "853152"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['00', '000', '01', '07', '10', '100', '11', '110', '110 free', '12',\n",
       "       ...\n",
       "       'yo', 'york', 'young', 'yourvoice', 'yourvoice america', 'youtube',\n",
       "       'youtube playlist', 'youtube video', 'zero', 'zip'],\n",
       "      dtype='object', length=1828)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform SVD to reduce dimensionality to about ~1000 (Currently ~1800)...Have to run an instance with about ~8GB of RAM (~0.08cents to perform one calculation); shut off the instance and restart this instance.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), \n",
    "                        stop_words = 'english', \n",
    "                        min_df = 25, \n",
    "                        max_df = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.SparseDataFrame(tfidf.fit_transform(df['lemm_clean_processed']),\n",
    "                        columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20699, 1019)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>13</th>\n",
       "      <th>130</th>\n",
       "      <th>130 free</th>\n",
       "      <th>...</th>\n",
       "      <th>year url</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube playlist</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1019 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   10  100   11   12  120   13  130  130 free ...   year url  yep  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0 ...        0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0 ...        0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0 ...        0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0 ...        0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0 ...        0.0  0.0   \n",
       "\n",
       "   yes  yesterday   yo  young  youtube  youtube playlist  youtube video  zip  \n",
       "0  0.0        0.0  0.0    0.0      0.0               0.0            0.0  0.0  \n",
       "1  0.0        0.0  0.0    0.0      0.0               0.0            0.0  0.0  \n",
       "2  0.0        0.0  0.0    0.0      0.0               0.0            0.0  0.0  \n",
       "3  0.0        0.0  0.0    0.0      0.0               0.0            0.0  0.0  \n",
       "4  0.0        0.0  0.0    0.0      0.0               0.0            0.0  0.0  \n",
       "\n",
       "[5 rows x 1019 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as data\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10));\n",
    "# plt.scatter(df_tfidf.T[0], df_tfidf.T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-26f33499744f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pip install hdbscan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2417\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pexpect-U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<pexpect factory incomplete>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 303\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptyproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_spawnpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# Use internal fork_pty, for Solaris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmaster_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Establish a new session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "! pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-3f5a460d7435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdbscan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-770ac7b94e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_cluster_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_min_span_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hdbscan' is not defined"
     ]
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size = 5, gen_min_span_tree = True)\n",
    "cluster.fit(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD = TruncatedSVD(n_components=1000) \n",
    "# #doesn't center out data...versus PCA which it does...\n",
    "# ##If we didn't fit before train_test_split (WHY DO WE NEED TO FIT TRANSFORM BEFORE TRAIN TEST SPLIT...)\n",
    "# svd_matrix = SVD.fit_transform(df_tfidf)\n",
    "# svd_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/6e/a89da6b5c83f8811e46e3a9270c1aed90e9b9ee6c60faf52b7239e5d3d69/spacy-2.0.18-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /opt/conda/lib/python3.6/site-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /opt/conda/lib/python3.6/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.29.0)\n",
      "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
      "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
      "Installing collected packages: spacy\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "! pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0f87f98b7209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpunctuations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Function to clean up text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# punctuations = string.punctuation\n",
    "\n",
    "# #Function to clean up text\n",
    "# def cleanup_text(docs, logging=False):\n",
    "#     texts = []\n",
    "#     counter = 1\n",
    "#     for doc in docs:\n",
    "#         if counter % 1000 == 0 and logging:\n",
    "#             print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "#         counter += 1\n",
    "#         doc = nlp(doc, disable=['parser', 'ner']) #disabling default models of spacy\n",
    "#         tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']  #Applyint tokenizing and lemmatizing\n",
    "#         tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations] #Removing stopwords and punctuations\n",
    "#         tokens = ' '.join(tokens) \n",
    "#         texts.append(tokens)  #Appending clean titles to texts list\n",
    "#     return pd.Series(texts)   #Returning texts list as pd Series\n",
    "\n",
    "# tweets_clean= cleanup_text(lemm_clean_processed_list)\n",
    "# tweets_clean = ' '.join(lemm_clean_processed_list).split()\n",
    "\n",
    "# tweets_counts = Counter(tweets_clean)\n",
    "\n",
    "# tweets_common_words = [word[0] for word in tweets_counts.most_common(20)]\n",
    "# tweets_common_counts = [word[1] for word in tweets_counts.most_common(20)]\n",
    "# fig = plt.figure(figsize=(18,6))\n",
    "# sns.barplot(x=tweets_common_words, y=tweets_common_counts, palette = \"GnBu_d\")\n",
    "# sns.set(font_scale=1.5, rc={'figure.figsize':(8, 8)})\n",
    "# plt.title('Most Common Words used in tweets')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
