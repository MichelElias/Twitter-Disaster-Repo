{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re, nltk, hdbscan, spacy, string\n",
    "import psycopg2 as pg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sys import getsizeof\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from psycopg2.extras import RealDictCursor, Json\n",
    "from spacy.lang.en.examples import sentences\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_format_log(file_path, \n",
    "                        logfile = 'assets/file_log.txt', \n",
    "                        now = round(time.time()), \n",
    "                        file_description = None): \n",
    "   \n",
    "    try:\n",
    "        ext = re.search('(?<!^)(?<!\\.)\\.(?!\\.)', file_path).start() \n",
    "    except:\n",
    "        raise NameError('Please enter a relative path with a file extension.') \n",
    "    \n",
    "    stamp = re.search('(?<!^)(?<!\\.)[a-z]+_[a-z]+(?=\\.)', file_path).start()\n",
    "    formatted_name = f'{file_path[:stamp]}{now}_{file_path[stamp:]}'  \n",
    "    if not file_description:\n",
    "        file_description = f'Word list saved at: {time.asctime(time.gmtime(now))}'\n",
    "    with open(logfile, 'a+') as f:\n",
    "        f.write(f'{formatted_name}: {file_description}\\n')\n",
    "    return formatted_name, now, file_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1547416339_crisislex_df.csv    file_log.txt\n",
      "1547416339_crisislex_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "!ls 'assets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the csv to format as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('assets/1547416339_crisislex_df.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>processed</th>\n",
       "      <th>clean_processed</th>\n",
       "      <th>lemm_clean_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i've got enough candles to supply a mexican fa...</td>\n",
       "      <td>['i', 've', 'got', 'enough', 'candles', 'to', ...</td>\n",
       "      <td>i ve got enough candle to supply a mexican family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>['sandy', 'be', 'soooo', 'mad', 'that', 'she',...</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "      <td>['ibexgirl', 'thankfully', 'hurricane', 'waugh...</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>taos you never got that magnificent case of bu...</td>\n",
       "      <td>['taos', 'you', 'never', 'got', 'that', 'magni...</td>\n",
       "      <td>tao you never got that magnificent case of bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i'm at mad river bar &amp;amp; grille (new york, n...</td>\n",
       "      <td>['i', 'm', 'at', 'mad', 'river', 'bar', 'amp',...</td>\n",
       "      <td>i m at mad river bar amp grille new york ny URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet id                                              tweet  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "  label       type                                          processed  \\\n",
       "0     0  hurricane  i've got enough candles to supply a mexican fa...   \n",
       "1     1  hurricane  sandy be soooo mad that she be shattering our ...   \n",
       "2     0  hurricane  ibexgirl thankfully hurricane waugh played it ...   \n",
       "3     0  hurricane  taos you never got that magnificent case of bu...   \n",
       "4     0  hurricane  i'm at mad river bar &amp; grille (new york, n...   \n",
       "\n",
       "                                     clean_processed  \\\n",
       "0  ['i', 've', 'got', 'enough', 'candles', 'to', ...   \n",
       "1  ['sandy', 'be', 'soooo', 'mad', 'that', 'she',...   \n",
       "2  ['ibexgirl', 'thankfully', 'hurricane', 'waugh...   \n",
       "3  ['taos', 'you', 'never', 'got', 'that', 'magni...   \n",
       "4  ['i', 'm', 'at', 'mad', 'river', 'bar', 'amp',...   \n",
       "\n",
       "                                lemm_clean_processed  \n",
       "0  i ve got enough candle to supply a mexican family  \n",
       "1  sandy be soooo mad that she be shattering our ...  \n",
       "2  ibexgirl thankfully hurricane waugh played it ...  \n",
       "3  tao you never got that magnificent case of bur...  \n",
       "4    i m at mad river bar amp grille new york ny URL  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet id                0\n",
       "tweet                   1\n",
       "label                   2\n",
       "type                    2\n",
       "processed               2\n",
       "clean_processed         2\n",
       "lemm_clean_processed    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 0, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[0]['lemm_clean_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spacy_data(dataset, feature_column):\n",
    "    '''\n",
    "    Grabs the verb, adverb, noun, and stop word Parts of Speech (POS) \n",
    "    tokens and pushes them into a new dataset. returns an \n",
    "    enriched dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    dataset (dataframe): the dataframe to parse\n",
    "    feature_column (string): the column to parse in the dataset.\n",
    "    \n",
    "    Returns: \n",
    "    dataframe\n",
    "    '''\n",
    "    \n",
    "    verbs = []\n",
    "    nouns = []\n",
    "    adverbs = []\n",
    "    corpus = []\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    ##\n",
    "    for i in range (len(dataset)):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Extracting verbs and topics from record {i+1} of {len(dataset)}\")\n",
    "        \n",
    "        tweet = dataset.iloc[i][feature_column]\n",
    "        doc = nlp(tweet)\n",
    "        spacy_dataframe = pd.DataFrame()\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.lemma_ == \"-PRON-\":\n",
    "                    lemma = token.text\n",
    "            else:\n",
    "                lemma = token.lemma_\n",
    "            row = {\n",
    "                \"Word\": token.text,\n",
    "                \"Lemma\": lemma,\n",
    "                \"PoS\": token.pos_,\n",
    "                \"Stop Word\": token.is_stop\n",
    "            }\n",
    "            spacy_dataframe = spacy_dataframe.append(row, ignore_index = True)\n",
    "        verbs.append(\" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"PoS\"] == \"VERB\"].values))\n",
    "        nouns.append(\" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"PoS\"] == \"NOUN\"].values))\n",
    "        adverbs.append(\" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"PoS\"] == \"ADV\"].values))\n",
    "        corpus_clean = \" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"Stop Word\"] == False].values)\n",
    "        corpus_clean = re.sub(r'[^A-Za-z0-9]+', ' ', corpus_clean)   \n",
    "        corpus.append(corpus_clean)\n",
    "    dataset['Verbs'] = verbs\n",
    "    dataset['Nouns'] = nouns\n",
    "    dataset['Adverbs'] = adverbs\n",
    "    dataset['Corpus'] = corpus\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def prep_corpus(raw_string):\n",
    "    '''Single use of add_spacy_data to enable pipelining \n",
    "    data into predictions\n",
    "    \n",
    "    Parameters:\n",
    "    raw_string (string): String to be parsed\n",
    "    \n",
    "    Returns:\n",
    "    parsed string\n",
    "    '''\n",
    "\n",
    "    verbs = []\n",
    "    nouns = []\n",
    "    adverbs = []\n",
    "    corpus = []\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    doc = nlp(raw_string)\n",
    "    spacy_dataframe = pd.DataFrame()\n",
    "    for token in doc:\n",
    "        if token.lemma_ == \"-PRON-\":\n",
    "                lemma = token.text\n",
    "        else:\n",
    "            lemma = token.lemma_\n",
    "        row = {\n",
    "            \"Word\": token.text,\n",
    "            \"Lemma\": lemma,\n",
    "            \"PoS\": token.pos_,\n",
    "            \"Stop Word\": token.is_stop\n",
    "        }\n",
    "        spacy_dataframe = spacy_dataframe.append(row, ignore_index = True)\n",
    "    verbs.append(\" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"PoS\"] == \"VERB\"].values))\n",
    "    nouns.append(\" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"PoS\"] == \"NOUN\"].values))\n",
    "    adverbs.append(\" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"PoS\"] == \"ADV\"].values))\n",
    "    corpus_clean = \" \".join(spacy_dataframe[\"Lemma\"][spacy_dataframe[\"Stop Word\"] == False].values)\n",
    "    corpus_clean = re.sub(r'[^A-Za-z0-9]+', ' ', corpus_clean)   \n",
    "\n",
    "    return corpus_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_popular_terms(dataset, feature_column, disaster_column):\n",
    "    '''Function that counts the frequency of occurences of words in a dataset\n",
    "    column. Returns a new dataset with those frequencies'''\n",
    "    frequencies = pd.DataFrame()\n",
    "    disaster = dataset[disaster_column].unique().tolist()\n",
    "    for i in range (len(disaster)):\n",
    "        disaster_corpus = str(dataset[feature_column][dataset[disaster_column] == disaster[i]].tolist())\n",
    "        tokens = disaster_corpus.split(\" \")\n",
    "        counts = Counter(tokens)\n",
    "        frequencies = frequencies.append({\n",
    "            \"disaster\": disaster[i],\n",
    "            \"Most Common Terms\": counts.most_common(n=100)\n",
    "        }, ignore_index=True)\n",
    "#     frequencies['disaster'] = frequencies['Most Common Terms'].astype(int)\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_common_words(dataset):\n",
    "    '''Maps common words from across multiple columns in a dataset to \n",
    "    identify terms that show up in all columns. Normally used with the \n",
    "    outputs of map_popular_terms. returns the common words'''\n",
    "    common_words = []\n",
    "    for words in dataset['Most Common Terms'][0]:\n",
    "        common_words.append(words[0])\n",
    "\n",
    "    for i in range (0, len(dataset)):\n",
    "        check_list = []\n",
    "        disaster_list = dataset['Most Common Terms'][i]\n",
    "        for words in disaster_list:\n",
    "            check_list.append(words[0])\n",
    "        common_words = [x for x in common_words if x  in check_list]\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_frequency(term_list, frequency_list):\n",
    "    '''Finds the frequency of occurence of terms in a list and then\n",
    "    returns them in a new dataframe organized by year'''\n",
    "    common_word_frequency_per_disaster = pd.DataFrame()\n",
    "    for i in range(0, len(term_list)):\n",
    "        word_frequency = []\n",
    "        for j in range(0, len(frequency_list)):\n",
    "            current_disaster = frequency_list['disaster'][j]\n",
    "            current_disaster_terms = frequency_list['Most Common Terms'][j]\n",
    "            for words in current_disaster_terms:\n",
    "                    if term_list[i] in words[0]:\n",
    "                        word_frequency.append(words[1])\n",
    "                        #print(words[1])\n",
    "                        break\n",
    "        current_word = term_list[i]\n",
    "        common_word_frequency_per_disaster[str(current_word)] = word_frequency\n",
    "    common_word_frequency_per_disaster[\"disaster\"] = np.arange(1970,2019)\n",
    "    common_word_frequency_per_disaster = common_word_frequency_per_disaster.set_index(\"v\")\n",
    "    return common_word_frequency_per_disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_spacy_data(df, 'lemm_clean_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>processed</th>\n",
       "      <th>clean_processed</th>\n",
       "      <th>lemm_clean_processed</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Adverbs</th>\n",
       "      <th>Corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i've got enough candles to supply a mexican fa...</td>\n",
       "      <td>['i', 've', 'got', 'enough', 'candles', 'to', ...</td>\n",
       "      <td>i ve got enough candle to supply a mexican family</td>\n",
       "      <td>ve get supply</td>\n",
       "      <td>candle family</td>\n",
       "      <td></td>\n",
       "      <td>i ve get enough candle to supply a mexican family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>['sandy', 'be', 'soooo', 'mad', 'that', 'she',...</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>be be shatter</td>\n",
       "      <td>door hurricanesandy</td>\n",
       "      <td>soooo</td>\n",
       "      <td>sandy be soooo mad that she be shatter our doo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "      <td>['ibexgirl', 'thankfully', 'hurricane', 'waugh...</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "      <td>play wait go</td>\n",
       "      <td>ibexgirl hurricane waugh one moment tho</td>\n",
       "      <td>thankfully</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh play it co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>taos you never got that magnificent case of bu...</td>\n",
       "      <td>['taos', 'you', 'never', 'got', 'that', 'magni...</td>\n",
       "      <td>tao you never got that magnificent case of bur...</td>\n",
       "      <td>get send thank</td>\n",
       "      <td>case tweet</td>\n",
       "      <td>never</td>\n",
       "      <td>tao you never get that magnificent case of bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i'm at mad river bar &amp;amp; grille (new york, n...</td>\n",
       "      <td>['i', 'm', 'at', 'mad', 'river', 'bar', 'amp',...</td>\n",
       "      <td>i m at mad river bar amp grille new york ny URL</td>\n",
       "      <td>m grille</td>\n",
       "      <td>river bar amp york ny url</td>\n",
       "      <td></td>\n",
       "      <td>i m at mad river bar amp grille new york ny url</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet id                                              tweet  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "  label       type                                          processed  \\\n",
       "0     0  hurricane  i've got enough candles to supply a mexican fa...   \n",
       "1     1  hurricane  sandy be soooo mad that she be shattering our ...   \n",
       "2     0  hurricane  ibexgirl thankfully hurricane waugh played it ...   \n",
       "3     0  hurricane  taos you never got that magnificent case of bu...   \n",
       "4     0  hurricane  i'm at mad river bar &amp; grille (new york, n...   \n",
       "\n",
       "                                     clean_processed  \\\n",
       "0  ['i', 've', 'got', 'enough', 'candles', 'to', ...   \n",
       "1  ['sandy', 'be', 'soooo', 'mad', 'that', 'she',...   \n",
       "2  ['ibexgirl', 'thankfully', 'hurricane', 'waugh...   \n",
       "3  ['taos', 'you', 'never', 'got', 'that', 'magni...   \n",
       "4  ['i', 'm', 'at', 'mad', 'river', 'bar', 'amp',...   \n",
       "\n",
       "                                lemm_clean_processed           Verbs  \\\n",
       "0  i ve got enough candle to supply a mexican family   ve get supply   \n",
       "1  sandy be soooo mad that she be shattering our ...   be be shatter   \n",
       "2  ibexgirl thankfully hurricane waugh played it ...    play wait go   \n",
       "3  tao you never got that magnificent case of bur...  get send thank   \n",
       "4    i m at mad river bar amp grille new york ny URL        m grille   \n",
       "\n",
       "                                     Nouns     Adverbs  \\\n",
       "0                            candle family               \n",
       "1                      door hurricanesandy       soooo   \n",
       "2  ibexgirl hurricane waugh one moment tho  thankfully   \n",
       "3                               case tweet       never   \n",
       "4                river bar amp york ny url               \n",
       "\n",
       "                                              Corpus  \n",
       "0  i ve get enough candle to supply a mexican family  \n",
       "1  sandy be soooo mad that she be shatter our doo...  \n",
       "2  ibexgirl thankfully hurricane waugh play it co...  \n",
       "3  tao you never get that magnificent case of bur...  \n",
       "4    i m at mad river bar amp grille new york ny url  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most Common Terms</th>\n",
       "      <th>disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(hurricane, 4838), (be, 3988), (the, 3851), (...</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(the, 8852), (to, 7542), (flood, 7231), (be, ...</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(to, 4190), (the, 4180), (be, 3199), (oklahom...</td>\n",
       "      <td>tornado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Most Common Terms   disaster\n",
       "0  [(hurricane, 4838), (be, 3988), (the, 3851), (...  hurricane\n",
       "1  [(the, 8852), (to, 7542), (flood, 7231), (be, ...      flood\n",
       "2  [(to, 4190), (the, 4180), (be, 3199), (oklahom...    tornado"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_popular_terms(df, 'Corpus', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
