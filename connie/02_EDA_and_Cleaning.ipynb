{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/connie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane = pd.read_csv('../Mike/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv')\n",
    "alberta_floods = pd.read_csv('../Mike/CrisisLexT6/2013_Alberta_Floods/2013_Alberta_Floods-ontopic_offtopic.csv')\n",
    "boston_bombing = pd.read_csv('../Mike/CrisisLexT6/2013_Boston_Bombings/2013_Boston_Bombings-ontopic_offtopic.csv')\n",
    "oklahoma_tornado = pd.read_csv('../Mike/CrisisLexT6/2013_Oklahoma_Tornado/2013_Oklahoma_Tornado-ontopic_offtopic.csv')\n",
    "queensland_flood = pd.read_csv('../Mike/CrisisLexT6/2013_Queensland_Floods/2013_Queensland_Floods-ontopic_offtopic.csv')\n",
    "texas_explosion = pd.read_csv('../Mike/CrisisLexT6/2013_West_Texas_Explosion/2013_West_Texas_Explosion-ontopic_offtopic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandy Hurricane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>on-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet id                                              tweet  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "       label  \n",
       "0  off-topic  \n",
       "1   on-topic  \n",
       "2  off-topic  \n",
       "3  off-topic  \n",
       "4  off-topic  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_hurricane.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane.rename(columns = {'tweet id': 'id',\n",
    "                                  ' tweet': 'text',\n",
    "                                  ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode column `on-topic` and save as `sandy_hurricane_df`: \n",
    "-  1 = on-topic, tweet is related to a disaster\n",
    "-  0 = off-topic, tweet is *not* related to a disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane_df = pd.get_dummies(data=sandy_hurricane, \n",
    "                                    columns=['label'], \n",
    "                                    drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `type` Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to indicate the type of disaster it is. This will be necessary when combining all dataframes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane_df['type'] = 'hurricane'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for all other labeled datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alberta Floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_floods.rename(columns = {'tweet id': 'id',\n",
    "                                 ' tweet': 'text',\n",
    "                                 ' label': 'label'},\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_floods_df = pd.get_dummies(data=alberta_floods, \n",
    "                                   columns=['label'], \n",
    "                                   drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_floods_df['type'] = 'flood'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Bombing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bombing.rename(columns = {'tweet id': 'id',\n",
    "                                 ' tweet': 'text',\n",
    "                                 ' label': 'label'},\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bombing_df = pd.get_dummies(data=boston_bombing, \n",
    "                                   columns=['label'], \n",
    "                                   drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bombing_df['type'] = 'bombing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oklahoma Tornado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "oklahoma_tornado.rename(columns = {'tweet id': 'id',\n",
    "                                   ' tweet': 'text',\n",
    "                                   ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oklahoma_tornado_df = pd.get_dummies(data=oklahoma_tornado, \n",
    "                                     columns=['label'], \n",
    "                                     drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oklahoma_tornado_df['type'] = 'tornado'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queensland Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "queensland_flood.rename(columns = {'tweet id': 'id',\n",
    "                                   ' tweet': 'text',\n",
    "                                   ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "queensland_flood_df = pd.get_dummies(data=queensland_flood, \n",
    "                                     columns=['label'], \n",
    "                                     drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "queensland_flood_df['type'] = 'flood'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texas Explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_explosion.rename(columns = {'tweet id': 'id',\n",
    "                                  ' tweet': 'text',\n",
    "                                  ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_explosion_df = pd.get_dummies(data=texas_explosion, \n",
    "                                    columns=['label'], \n",
    "                                    drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_explosion_df['type'] = 'explosion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all processed dataframes into one comprehensive dataframe with each tweet related to its respective disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df = pd.concat([sandy_hurricane_df, \n",
    "                             alberta_floods_df, \n",
    "                             oklahoma_tornado_df, \n",
    "                             queensland_flood_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40064, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labeled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 60,082 observations and 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_on-topic</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "   label_on-topic       type  \n",
       "0               0  hurricane  \n",
       "1               1  hurricane  \n",
       "2               0  hurricane  \n",
       "3               0  hurricane  \n",
       "4               0  hurricane  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text\n",
    "\n",
    "In order to remove any text that will only contribute noise to our model, we will define a function that uses regular expressions to replace certain patterns:\n",
    "\n",
    "-  **Convert all text to lower case**\n",
    "-  **Remove additional white sapce**\n",
    "-  **Remove links:** \n",
    "    -  Links starting with `www.` or `https?://:` are replaced with `URL`. Each link is most likely to be unique to the tweet and won't provide any information in regards to the content overall. \n",
    "-  **Eliminate hashtags**\n",
    "-  **Remove `@`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet = re.sub('@', '', tweet)\n",
    "    tweet = re.sub('rt', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df['processed'] = [processTweet(i) for i in final_labeled_df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate tokenizer and define the search pattern using `r'\\w+` as our regular expression. We only want to search through words and omit digits and symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the `processed` column and create a new column (`tokenized`) for our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df['tokenized'] = final_labeled_df['processed'].map(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will lemmatize our data in an attempt to return the base form of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lemmatize the tokenized words in `tokenized` and join them to represent one string again. This is shown in the `lemmatized` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df['lemmatized'] = final_labeled_df['tokenized'].map(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_on-topic</th>\n",
       "      <th>type</th>\n",
       "      <th>processed</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i've got enough candles to supply a mexican fa...</td>\n",
       "      <td>[i, ve, got, enough, candles, to, supply, a, m...</td>\n",
       "      <td>i ve got enough candle to supply a mexican family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>[sandy, be, soooo, mad, that, she, be, shatter...</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "      <td>[ibexgirl, thankfully, hurricane, waugh, playe...</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>taos you never got that magnificent case of bu...</td>\n",
       "      <td>[taos, you, never, got, that, magnificent, cas...</td>\n",
       "      <td>tao you never got that magnificent case of bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i'm at mad river bar &amp;amp; grille (new york, ny)</td>\n",
       "      <td>[i, m, at, mad, river, bar, amp, grille, new, ...</td>\n",
       "      <td>i m at mad river bar amp grille new york ny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "   label_on-topic       type  \\\n",
       "0               0  hurricane   \n",
       "1               1  hurricane   \n",
       "2               0  hurricane   \n",
       "3               0  hurricane   \n",
       "4               0  hurricane   \n",
       "\n",
       "                                           processed  \\\n",
       "0  i've got enough candles to supply a mexican fa...   \n",
       "1  sandy be soooo mad that she be shattering our ...   \n",
       "2  ibexgirl thankfully hurricane waugh played it ...   \n",
       "3  taos you never got that magnificent case of bu...   \n",
       "4  i'm at mad river bar &amp; grille (new york, ny)    \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [i, ve, got, enough, candles, to, supply, a, m...   \n",
       "1  [sandy, be, soooo, mad, that, she, be, shatter...   \n",
       "2  [ibexgirl, thankfully, hurricane, waugh, playe...   \n",
       "3  [taos, you, never, got, that, magnificent, cas...   \n",
       "4  [i, m, at, mad, river, bar, amp, grille, new, ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  i ve got enough candle to supply a mexican family  \n",
       "1  sandy be soooo mad that she be shattering our ...  \n",
       "2  ibexgirl thankfully hurricane waugh played it ...  \n",
       "3  tao you never got that magnificent case of bur...  \n",
       "4        i m at mad river bar amp grille new york ny  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df.to_csv('./data/final_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to reduce the dimension of our feature space from the 2599 that we currently have. SVD will be used to determine how all the features relate to one another and find the top 1000 combination of features that explain the most varaince. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c3ea862864db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSVD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvd_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msvd_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    176\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           random_state=random_state)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown algorithm %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 326\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# Sample the range of A using by linear projection of Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# Extract an orthonormal basis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'economic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/linalg/decomp_qr.py\u001b[0m in \u001b[0;36mqr\u001b[0;34m(a, overwrite_a, lwork, mode, pivoting, check_finite)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'economic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         Q, = safecall(gor_un_gqr, \"gorgqr/gungqr\", qr, tau, lwork=lwork,\n\u001b[0;32m--> 165\u001b[0;31m                       overwrite_a=1)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/linalg/decomp_qr.py\u001b[0m in \u001b[0;36msafecall\u001b[0;34m(f, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lwork'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         raise ValueError(\"illegal value in %d-th argument of internal %s\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SVD = TruncatedSVD(n_components=1000) \n",
    "svd_matrix = SVD.fit_transform(tfidf_df)\n",
    "svd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.bar(np.array(range(1000))+1, \n",
    "        SVD.explained_variance_ratio_, \n",
    "        color='g', \n",
    "        label='Explained Variance')\n",
    "plt.plot(np.array(range(1000))+1, \n",
    "         np.cumsum(SVD.explained_variance_ratio_), \n",
    "         label='Cumulative Explained Variance')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('Component', fontsize=20)\n",
    "plt.ylabel('Variance Ratio', fontsize=20)\n",
    "plt.title('Explained Variance by Component', fontsize=36);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_names = pd.Series([\"component_\"+str(i) for i in range(1, 1001)])\n",
    "svd_df = pd.SparseDataFrame(svd_matrix,\n",
    "                            columns=component_names)\n",
    "svd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.SparseDataFrame(SVD.components_,\n",
    "                              index=component_names,\n",
    "                              columns=tfidf_df.columns).T\n",
    "loadings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings['abs_component_1'] = np.abs(loadings.component_1)\n",
    "loadings['abs_component_2'] = np.abs(loadings.component_2)\n",
    "loadings['abs_component_3'] = np.abs(loadings.component_3)\n",
    "loadings['abs_component_4'] = np.abs(loadings.component_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings.sort_values('abs_component_1',ascending=False).head(20)[['component_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization - CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range = (1,2),\n",
    "                     stop_words = 'english',\n",
    "                     min_df = 15,\n",
    "                     max_df = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.SparseDataFrame(cv.fit_transform(final_labeled_df['lemmatized']), \n",
    "                           columns = cv.get_feature_names())\n",
    "cv_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.barh(y = cv_df.sum().sort_values(ascending=False).head(20).index,\n",
    "         width = cv_df.sum().sort_values(ascending=False).head(20))\n",
    "plt.title('Top Word Count', fontsize=30)\n",
    "plt.xlabel('Count', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
